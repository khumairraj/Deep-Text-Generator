{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils.data_utils import get_file\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import string\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the file location\n",
    "path = 'Gullivers_Travel'\n",
    "text = open(path, encoding ='utf').read().lower()\n",
    "text= text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #Removing undesirable words and characters\n",
    "# # print ('Initial---------')\n",
    "# # print('Corpus Length', len(text))\n",
    "# # chars = sorted(list(set(text)))\n",
    "# # print('Total character types:', len(chars))\n",
    "# # bad_words = ['www.xtratutors.com']\n",
    "# # with open('pg14916.txt') as oldfile, open('pg14916_new.txt', 'w') as newfile:\n",
    "# #     for line in oldfile:\n",
    "# #         if not any(bad_word in line for bad_word in bad_words):\n",
    "# #             line= line.translate(str.maketrans('','',\"\\n\"string.punctuation))\n",
    "# #             newfile.write(line)\n",
    "            \n",
    "# # path = 'pg14916_new.txt'\n",
    "# # text = open(path).read().lower()\n",
    "# print ('Final---------')\n",
    "# print('Corpus Length', len(text))\n",
    "# chars = sorted(list(set(text)))\n",
    "# print('Total character types:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "#Define Dictionary For Converting to One Hot.\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 552095\n"
     ]
    }
   ],
   "source": [
    "#Converting to the trainble matrix size\n",
    "maxlen = 100\n",
    "step = 1\n",
    "lettergroup = []\n",
    "nextletter = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    lettergroup.append(text[i: i + maxlen])\n",
    "    nextletter.append(text[i + maxlen])\n",
    "print('nb sequences:', len(lettergroup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Convert to One Hot\n",
    "X = np.zeros((len(lettergroup), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(lettergroup), len(chars)), dtype=np.bool)\n",
    "for i, group in enumerate(lettergroup):\n",
    "    for t, char in enumerate(group):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[nextletter[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Building The LSTM Model\n",
    "# #model1\n",
    "# model = Sequential()\n",
    "# model.add(LSTM(512, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(LSTM(512))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add((Dense(len(chars), activation='softmax')))\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building The LSTM Model\n",
    "#model2\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars)),return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add((Dense(len(chars), activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath=\"weights-improvement-{epoch:02d}-{loss:4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath,monitor='loss',verbose=1,save_best_only=True, mode='min')\n",
    "callbacks_list=[checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Temperature...Helper function to sample an index from a probability array...\n",
    "def sample(preds, temperature=1):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Epoch 1/1\n",
      "552064/552095 [============================>.] - ETA: 0s - loss: 1.8798Epoch 00000: loss improved from inf to 1.87977, saving model to weights-improvement-00-1.879768.hdf5\n",
      "552095/552095 [==============================] - 2002s - loss: 1.8798  \n",
      "\n",
      "----- diversity: 0.2\n",
      "----- Generating with seed: \" just daylight  i attempted to rise but was not able to stir for as i happened to lie on my back i f\"\n",
      " just daylight  i attempted to rise but was not able to stir for as i happened to lie on my back i for the strong and the strong the strong and the strong and the strong and the strong and the strong the strong of the strong my hand of the strong of the strenges of the strect of the strong and those which we had a served the strong the strong the strong of the strong of the streng of the strong that i had not be a should be signs of the sea of the strong of the strong of the streng and the stron\n",
      "\n",
      "----- diversity: 0.5\n",
      "----- Generating with seed: \" just daylight  i attempted to rise but was not able to stir for as i happened to lie on my back i f\"\n",
      " just daylight  i attempted to rise but was not able to stir for as i happened to lie on my back i fear able to be the first high with he was many should for which was not store the prosed about the short of his fignt to be wholly came of the digner to the spale of the wind should perpeced the other which beard the among this proposed the wind me becaure that the stregt my whole country and senved my propocity of would be the stred to the was someting in the master of the came of his seeping the\n",
      "\n",
      "----- diversity: 1.0\n",
      "----- Generating with seed: \" just daylight  i attempted to rise but was not able to stir for as i happened to lie on my back i f\"\n",
      " just daylight  i attempted to rise but was not able to stir for as i happened to lie on my back i forded \n",
      "my comm in a mhen who was retered the monions could which a thing the courtme with his larsrs and the languagers left howed the wares and thought feet her more as the yahoos in hipker and then houdgnanch de in went might me unverseished the three and of the might ymulf boable have as a condin nezern my miscorwed his lowgan excrise not by compliden person upun the time i coused  the sour pro\n",
      "\n",
      "----- diversity: 1.2\n",
      "----- Generating with seed: \" just daylight  i attempted to rise but was not able to stir for as i happened to lie on my back i f\"\n",
      " just daylight  i attempted to rise but was not able to stir for as i happened to lie on my back i fahow and blef a in the ala king’s  is ownfs of thar suse it found sheld gienes is brined the des the muninaistienst ippecoom bedsonable agy or got as said with i he hudgen as ped theiewer endsail  ““tonte the pyapoos clawtipledcost ofns of than resinure zalf beribly very confeler intenviet it they brenglures was several of that the lihe and haight bun one than this sea of my use pofteroy any higho\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Epoch 1/1\n",
      "278016/552095 [==============>...............] - ETA: 811s - loss: 1.4879"
     ]
    }
   ],
   "source": [
    "#Model Training And Output Generation After Each Step\n",
    "epochstot=20\n",
    "for iteration in range(1, epochstot):\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(X, y,batch_size=128, epochs=1, callbacks=callbacks_list)\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print()\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        print('----- Generating with seed: \"' + sentence + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x = np.zeros((1, maxlen, len(chars)))\n",
    "            for t, char in enumerate(sentence):\n",
    "                x[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Call a trained model\n",
    "# filename = \"/home/nick/Desktop/My Space/Wonderland/weights-improvement-00-3.009437.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
